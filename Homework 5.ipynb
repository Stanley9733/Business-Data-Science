{"cells":[{"cell_type":"markdown","metadata":{},"source":["## Business Data Science Homework 5\n","### Finished by Zhuo Wen & Hao Lun Chu Colin"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":"import pandas as pd\nimport numpy as np\nimport warnings\nwarnings.filterwarnings('ignore')"},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>stops</th>\n      <th>pop</th>\n      <th>past.arrests</th>\n      <th>precinct</th>\n      <th>eth</th>\n      <th>crime</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>75</td>\n      <td>1720</td>\n      <td>191</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>36</td>\n      <td>1720</td>\n      <td>57</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>74</td>\n      <td>1720</td>\n      <td>599</td>\n      <td>1</td>\n      <td>1</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>17</td>\n      <td>1720</td>\n      <td>133</td>\n      <td>1</td>\n      <td>1</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>37</td>\n      <td>1368</td>\n      <td>62</td>\n      <td>1</td>\n      <td>2</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <td>895</td>\n      <td>2</td>\n      <td>3233</td>\n      <td>4</td>\n      <td>75</td>\n      <td>2</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <td>896</td>\n      <td>111</td>\n      <td>61692</td>\n      <td>181</td>\n      <td>75</td>\n      <td>3</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <td>897</td>\n      <td>48</td>\n      <td>61692</td>\n      <td>38</td>\n      <td>75</td>\n      <td>3</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <td>898</td>\n      <td>115</td>\n      <td>61692</td>\n      <td>61</td>\n      <td>75</td>\n      <td>3</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <td>899</td>\n      <td>21</td>\n      <td>61692</td>\n      <td>32</td>\n      <td>75</td>\n      <td>3</td>\n      <td>4</td>\n    </tr>\n  </tbody>\n</table>\n<p>900 rows × 6 columns</p>\n</div>","text/plain":"     stops    pop  past.arrests  precinct  eth  crime\n0       75   1720           191         1    1      1\n1       36   1720            57         1    1      2\n2       74   1720           599         1    1      3\n3       17   1720           133         1    1      4\n4       37   1368            62         1    2      1\n..     ...    ...           ...       ...  ...    ...\n895      2   3233             4        75    2      4\n896    111  61692           181        75    3      1\n897     48  61692            38        75    3      2\n898    115  61692            61        75    3      3\n899     21  61692            32        75    3      4\n\n[900 rows x 6 columns]"},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":"data = pd.read_csv('NYC_stop_and_frisk.dat',sep=' ',skiprows=6)\ndata"},{"cell_type":"markdown","metadata":{},"source":"## 2. What fraction of the total stops correspond to “white/back/hispanic”? What fraction of the population corresponds to “white/black/hispanic”?"},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"The fraction of the total stops correspond to Black is 0.5312966063004109\nThe fraction of the total population correspond to Black is 0.27546911368656585\n\nThe fraction of the total stops correspond to Hispanic is 0.3395449703241516\nThe fraction of the total population correspond to Hispanic is 0.2557414035070107\n\nThe fraction of the total stops correspond to White is 0.12915842337543754\nThe fraction of the total population correspond to White is 0.46878948280642346\n\n"}],"source":"totalstop = data['stops'].sum()\ntotalpopulation = data['pop'].sum()\neth=['','Black','Hispanic','White']\nfor x in data['eth'].unique():\n    ethstop = data.loc[data.eth==x,'stops'].sum()\n    ethpopulation = data.loc[data.eth==x,'pop'].sum()\n    print('The fraction of the total stops correspond to',eth[x],'is',ethstop/totalstop)\n    print('The fraction of the total population correspond to',eth[x],'is',ethpopulation/totalpopulation)\n    print()"},{"cell_type":"markdown","metadata":{},"source":"## 3. Use a Poisson regression to model the number of stops, controlling for ethnicity and using the number of past arrests as an exposure input."},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":"WARNING (theano.configdefaults): g++ not available, if using conda: `conda install m2w64-toolchain`\nWARNING (theano.configdefaults): g++ not detected ! Theano will be unable to execute optimized C-implementations (for both CPU and GPU) and will default to Python implementations. Performance will be severely degraded. To remove this warning, set Theano flags cxx to an empty string.\nWARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n"}],"source":"import patsy as pt\nimport pymc3 as pm\nimport pandas as pd\nfrom patsy import dmatrices\nimport numpy as np\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt"},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>stops</th>\n      <th>pop</th>\n      <th>arrests</th>\n      <th>precinct</th>\n      <th>crime</th>\n      <th>black</th>\n      <th>hispanic</th>\n      <th>white</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>75</td>\n      <td>1720</td>\n      <td>191</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>36</td>\n      <td>1720</td>\n      <td>57</td>\n      <td>1</td>\n      <td>2</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>74</td>\n      <td>1720</td>\n      <td>599</td>\n      <td>1</td>\n      <td>3</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>17</td>\n      <td>1720</td>\n      <td>133</td>\n      <td>1</td>\n      <td>4</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>37</td>\n      <td>1368</td>\n      <td>62</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>","text/plain":"   stops   pop  arrests  precinct  crime  black  hispanic  white\n0     75  1720      191         1      1      1         0      0\n1     36  1720       57         1      2      1         0      0\n2     74  1720      599         1      3      1         0      0\n3     17  1720      133         1      4      1         0      0\n4     37  1368       62         1      1      0         1      0"},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":"df = pd.get_dummies(data, columns=['eth'])\ndf = df.rename(columns={'eth_1': \"black\", 'eth_2': \"hispanic\", 'eth_3': \"white\",'past.arrests':\"arrests\"})\ndf = df[df['arrests']>0]\ndf.head()"},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:                  stops   No. Observations:                  899\nModel:                            GLM   Df Residuals:                      895\nModel Family:                 Poisson   Df Model:                            3\nLink Function:                    log   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -94255.\nDate:                Thu, 17 Oct 2019   Deviance:                   1.8317e+05\nTime:                        23:52:51   Pearson chi2:                 2.79e+05\nNo. Iterations:                     6                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          z      P>|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     -0.8004      0.009    -89.294      0.000      -0.818      -0.783\nblack          0.1731      0.009     20.073      0.000       0.156       0.190\nhispanic       0.2464      0.009     27.021      0.000       0.229       0.264\npop         8.999e-07    8.1e-08     11.112      0.000    7.41e-07    1.06e-06\n==============================================================================\n"}],"source":"expr = \"\"\"stops ~ black + hispanic + pop\"\"\"\ny, X = dmatrices(expr, df, return_type='dataframe')\npoisson_training_results = sm.GLM(y, X, family=sm.families.Poisson(), exposure = df['arrests']).fit()\nprint(poisson_training_results.summary())"},{"cell_type":"markdown","metadata":{},"source":"### 4. According to the output of your model, what fraction fewer or more stops does each ethnicity have with respect to the others, in proportion to arrest rates of the previous year? Note that you can just pick a baseline ethnicity and just compare everything to that."},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"black = 1.1889612578195912\nhispanic = 1.279474797670672\nhispanic & white  diff = 1.279474797670672\nblack & white diff = 1.1889612578195912\n"}],"source":"black = np.exp(poisson_training_results._results.params['x1'])\nhispanic = np.exp(poisson_training_results._results.params['x2'])\nprint('black = '+str(black))\nprint('hispanic = '+str(hispanic))\nprint('hispanic & white  diff = '+ str(hispanic))\nprint('black & white diff = '+ str(black))"},{"cell_type":"markdown","metadata":{},"source":"### Compare to white, hispanic are 1.2794 more likely to get stopped and black are 1.188 more likely to get stopped."},{"cell_type":"markdown","metadata":{},"source":["### 5. Next, add the 75 precincts, and again solve the Poisson regression model."]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>stops</th>\n      <th>pop</th>\n      <th>arrests</th>\n      <th>crime</th>\n      <th>black</th>\n      <th>hispanic</th>\n      <th>white</th>\n      <th>precinct_1</th>\n      <th>precinct_2</th>\n      <th>precinct_3</th>\n      <th>...</th>\n      <th>precinct_66</th>\n      <th>precinct_67</th>\n      <th>precinct_68</th>\n      <th>precinct_69</th>\n      <th>precinct_70</th>\n      <th>precinct_71</th>\n      <th>precinct_72</th>\n      <th>precinct_73</th>\n      <th>precinct_74</th>\n      <th>precinct_75</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>75</td>\n      <td>1720</td>\n      <td>191</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>36</td>\n      <td>1720</td>\n      <td>57</td>\n      <td>2</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>74</td>\n      <td>1720</td>\n      <td>599</td>\n      <td>3</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>17</td>\n      <td>1720</td>\n      <td>133</td>\n      <td>4</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>37</td>\n      <td>1368</td>\n      <td>62</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 82 columns</p>\n</div>","text/plain":"   stops   pop  arrests  crime  black  hispanic  white  precinct_1  \\\n0     75  1720      191      1      1         0      0           1   \n1     36  1720       57      2      1         0      0           1   \n2     74  1720      599      3      1         0      0           1   \n3     17  1720      133      4      1         0      0           1   \n4     37  1368       62      1      0         1      0           1   \n\n   precinct_2  precinct_3  ...  precinct_66  precinct_67  precinct_68  \\\n0           0           0  ...            0            0            0   \n1           0           0  ...            0            0            0   \n2           0           0  ...            0            0            0   \n3           0           0  ...            0            0            0   \n4           0           0  ...            0            0            0   \n\n   precinct_69  precinct_70  precinct_71  precinct_72  precinct_73  \\\n0            0            0            0            0            0   \n1            0            0            0            0            0   \n2            0            0            0            0            0   \n3            0            0            0            0            0   \n4            0            0            0            0            0   \n\n   precinct_74  precinct_75  \n0            0            0  \n1            0            0  \n2            0            0  \n3            0            0  \n4            0            0  \n\n[5 rows x 82 columns]"},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":"df = pd.get_dummies(df, columns=['precinct'])\ndf = df[df['arrests']>1]\ndf.head()"},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:                  stops   No. Observations:                  898\nModel:                            GLM   Df Residuals:                      820\nModel Family:                 Poisson   Df Model:                           77\nLink Function:                    log   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -72931.\nDate:                Thu, 17 Oct 2019   Deviance:                   1.4053e+05\nTime:                        23:53:12   Pearson chi2:                 2.15e+05\nNo. Iterations:                     7                                         \nCovariance Type:            nonrobust                                         \n===============================================================================\n                  coef    std err          z      P>|z|      [0.025      0.975]\n-------------------------------------------------------------------------------\nIntercept      -1.1278      0.013    -83.904      0.000      -1.154      -1.101\npop          3.806e-06   1.42e-07     26.814      0.000    3.53e-06    4.08e-06\nblack           0.5785      0.012     49.938      0.000       0.556       0.601\nhispanic        0.5658      0.011     50.471      0.000       0.544       0.588\nprecinct_1     -0.8194      0.051    -16.186      0.000      -0.919      -0.720\nprecinct_2     -0.9636      0.053    -18.080      0.000      -1.068      -0.859\nprecinct_3     -0.2853      0.025    -11.337      0.000      -0.335      -0.236\nprecinct_4      0.3519      0.027     13.174      0.000       0.300       0.404\nprecinct_5     -0.5722      0.025    -22.766      0.000      -0.622      -0.523\nprecinct_6      0.3257      0.028     11.632      0.000       0.271       0.381\nprecinct_7     -0.6511      0.039    -16.702      0.000      -0.727      -0.575\nprecinct_8     -1.2165      0.026    -47.690      0.000      -1.266      -1.167\nprecinct_9     -0.3643      0.059     -6.210      0.000      -0.479      -0.249\nprecinct_10    -0.4115      0.030    -13.933      0.000      -0.469      -0.354\nprecinct_11    -0.3712      0.035    -10.752      0.000      -0.439      -0.304\nprecinct_12     0.2775      0.033      8.293      0.000       0.212       0.343\nprecinct_13     0.1324      0.021      6.222      0.000       0.091       0.174\nprecinct_14    -0.2926      0.029    -10.041      0.000      -0.350      -0.236\nprecinct_15     0.1632      0.021      7.906      0.000       0.123       0.204\nprecinct_16    -0.0712      0.032     -2.218      0.027      -0.134      -0.008\nprecinct_17    -0.9770      0.032    -30.193      0.000      -1.040      -0.914\nprecinct_18    -0.7907      0.021    -38.317      0.000      -0.831      -0.750\nprecinct_19    -0.7128      0.028    -25.650      0.000      -0.767      -0.658\nprecinct_20    -0.9743      0.025    -38.670      0.000      -1.024      -0.925\nprecinct_21    -0.5955      0.025    -23.786      0.000      -0.645      -0.546\nprecinct_22     0.2529      0.016     15.803      0.000       0.222       0.284\nprecinct_23    -0.2822      0.021    -13.508      0.000      -0.323      -0.241\nprecinct_24     0.3952      0.020     20.154      0.000       0.357       0.434\nprecinct_25    -0.1618      0.017     -9.318      0.000      -0.196      -0.128\nprecinct_26    -1.1733      0.026    -45.892      0.000      -1.223      -1.123\nprecinct_27     1.0066      0.022     44.947      0.000       0.963       1.051\nprecinct_28    -1.7970      0.033    -54.094      0.000      -1.862      -1.732\nprecinct_29     0.0542      0.019      2.793      0.005       0.016       0.092\nprecinct_30    -0.3931      0.022    -18.212      0.000      -0.435      -0.351\nprecinct_31     0.7563      0.024     32.104      0.000       0.710       0.802\nprecinct_32     0.5003      0.031     16.214      0.000       0.440       0.561\nprecinct_33     0.1051      0.019      5.672      0.000       0.069       0.141\nprecinct_34     0.6229      0.019     32.792      0.000       0.586       0.660\nprecinct_35    -0.1303      0.037     -3.508      0.000      -0.203      -0.058\nprecinct_36     0.5926      0.029     20.218      0.000       0.535       0.650\nprecinct_37     0.5052      0.031     16.039      0.000       0.443       0.567\nprecinct_38     0.8274      0.024     35.177      0.000       0.781       0.873\nprecinct_39    -0.8562      0.026    -33.389      0.000      -0.906      -0.806\nprecinct_40     0.5919      0.034     17.378      0.000       0.525       0.659\nprecinct_41     1.0480      0.023     46.126      0.000       1.003       1.092\nprecinct_42     0.0278      0.020      1.398      0.162      -0.011       0.067\nprecinct_43    -0.5803      0.026    -22.663      0.000      -0.631      -0.530\nprecinct_44    -0.0984      0.025     -3.998      0.000      -0.147      -0.050\nprecinct_45    -0.2316      0.019    -12.038      0.000      -0.269      -0.194\nprecinct_46    -0.4286      0.018    -23.842      0.000      -0.464      -0.393\nprecinct_47     0.2734      0.032      8.665      0.000       0.212       0.335\nprecinct_48    -0.7006      0.026    -26.902      0.000      -0.752      -0.650\nprecinct_49     0.2535      0.031      8.179      0.000       0.193       0.314\nprecinct_50    -0.1348      0.019     -7.278      0.000      -0.171      -0.098\nprecinct_51    -0.6883      0.029    -23.792      0.000      -0.745      -0.632\nprecinct_52    -0.4977      0.022    -22.779      0.000      -0.541      -0.455\nprecinct_53    -0.2492      0.029     -8.665      0.000      -0.306      -0.193\nprecinct_54    -0.4535      0.032    -14.375      0.000      -0.515      -0.392\nprecinct_55    -0.4708      0.030    -15.948      0.000      -0.529      -0.413\nprecinct_56     0.0600      0.042      1.441      0.150      -0.022       0.142\nprecinct_57     0.6921      0.034     20.281      0.000       0.625       0.759\nprecinct_58     0.6517      0.019     33.437      0.000       0.614       0.690\nprecinct_59     0.1913      0.029      6.507      0.000       0.134       0.249\nprecinct_60    -0.1767      0.019     -9.399      0.000      -0.214      -0.140\nprecinct_61     0.3250      0.025     13.102      0.000       0.276       0.374\nprecinct_62     0.0828      0.026      3.228      0.001       0.033       0.133\nprecinct_63     0.4090      0.028     14.517      0.000       0.354       0.464\nprecinct_64     0.8121      0.027     29.960      0.000       0.759       0.865\nprecinct_65     1.0124      0.021     47.998      0.000       0.971       1.054\nprecinct_66     1.1258      0.019     60.558      0.000       1.089       1.162\nprecinct_67     0.2549      0.020     12.456      0.000       0.215       0.295\nprecinct_68     1.2829      0.030     42.729      0.000       1.224       1.342\nprecinct_69     0.8355      0.028     29.778      0.000       0.780       0.890\nprecinct_70    -0.1469      0.022     -6.623      0.000      -0.190      -0.103\nprecinct_71     0.5216      0.019     28.075      0.000       0.485       0.558\nprecinct_72     0.4753      0.017     27.767      0.000       0.442       0.509\nprecinct_73     0.0594      0.017      3.562      0.000       0.027       0.092\nprecinct_74     0.0526      0.028      1.855      0.064      -0.003       0.108\nprecinct_75     0.6845      0.055     12.349      0.000       0.576       0.793\n===============================================================================\n"}],"source":"expr = \"\"\"stops ~ pop + black + hispanic + precinct_1 + precinct_2 + precinct_3 + precinct_4 + precinct_5 + precinct_6 + precinct_7 + precinct_8 + precinct_9 + precinct_10 + precinct_11 + precinct_12 + precinct_13 + precinct_14 + precinct_15 + precinct_16 + precinct_17 + precinct_18 + precinct_19 + precinct_20 + precinct_21 + precinct_22 + precinct_23 + precinct_24 + precinct_25 + precinct_26 + precinct_27 + precinct_28 + precinct_29 + precinct_30 + precinct_31 + precinct_32 + precinct_33 + precinct_34 + precinct_35 + precinct_36 + precinct_37 + precinct_38 + precinct_39 + precinct_40 + precinct_41 + precinct_42 + precinct_43 + precinct_44 + precinct_45 + precinct_46 + precinct_47 + precinct_48 + precinct_49 + precinct_50 + precinct_51 + precinct_52 + precinct_53 + precinct_54 + precinct_55 + precinct_56 + precinct_57 + precinct_58 + precinct_59 + precinct_60 + precinct_61 + precinct_62 + precinct_63 + precinct_64 + precinct_65 + precinct_66 + precinct_67 + precinct_68 + precinct_69 + precinct_70 + precinct_71 + precinct_72 + precinct_73 + precinct_74 + precinct_75\"\"\"\ny, X = dmatrices(expr, df, return_type='dataframe')\npoisson_training_results = sm.GLM(y, X, family=sm.families.Poisson(), exposure = df['arrests']).fit()\nprint(poisson_training_results.summary())"},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"black = 1.7834025417699364\nhispanic = 1.7607759094171689\nhispanic & white diff = 1.7607759094171689\nblack & white diff = 1.7834025417699364\n"}],"source":"black = np.exp(poisson_training_results._results.params['x2'])\nhispanic = np.exp(poisson_training_results._results.params['x3'])\nprint('black = '+str(black))\nprint('hispanic = '+str(hispanic))\nprint('hispanic & white diff = '+ str(hispanic))\nprint('black & white diff = '+ str(black))"},{"cell_type":"markdown","metadata":{},"source":"### After considering the precincts, by having whote as the baseline, hispanic are 1.7607 likely to get stopped, whereas black are 1.7834 less likely to get stopped."},{"cell_type":"markdown","metadata":{},"source":["## Problem 2"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":"from sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import r2_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.preprocessing import StandardScaler\nimport math"},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Comp</th>\n      <th>Height</th>\n      <th>Points</th>\n      <th>Salary</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>9.0</td>\n      <td>76.0</td>\n      <td>27.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>7.0</td>\n      <td>78.0</td>\n      <td>39.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>9.0</td>\n      <td>76.0</td>\n      <td>39.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>9.0</td>\n      <td>74.0</td>\n      <td>39.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>9.0</td>\n      <td>74.0</td>\n      <td>26.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <td>9995</td>\n      <td>8.0</td>\n      <td>76.0</td>\n      <td>43.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <td>9996</td>\n      <td>8.0</td>\n      <td>78.0</td>\n      <td>25.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <td>9997</td>\n      <td>9.0</td>\n      <td>77.0</td>\n      <td>31.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <td>9998</td>\n      <td>8.0</td>\n      <td>74.0</td>\n      <td>37.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <td>9999</td>\n      <td>9.0</td>\n      <td>78.0</td>\n      <td>31.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>10000 rows × 4 columns</p>\n</div>","text/plain":"      Comp  Height  Points  Salary\n0      9.0    76.0    27.0     0.0\n1      7.0    78.0    39.0     0.0\n2      9.0    76.0    39.0     0.0\n3      9.0    74.0    39.0     0.0\n4      9.0    74.0    26.0     0.0\n...    ...     ...     ...     ...\n9995   8.0    76.0    43.0     0.0\n9996   8.0    78.0    25.0     0.0\n9997   9.0    77.0    31.0     0.0\n9998   8.0    74.0    37.0     0.0\n9999   9.0    78.0    31.0     0.0\n\n[10000 rows x 4 columns]"},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":"nba = pd.read_csv('nba_cc_fake_data.csv', index_col=0)\nnba"},{"cell_type":"markdown","metadata":{},"source":"### 1. Explain why linear regression is not appropriate, given the nature of the data."},{"cell_type":"markdown","metadata":{},"source":"Linear regression is not appropriate here since there is no linear relationship between three features we are given and the salary. No matter how good a player is in high school, his salary will be zero if he didn't go to NBA. Besides that,all three features we have are not continuous value but are discrete or categorical variablem, making linear regression less accurate and meaningful."},{"cell_type":"markdown","metadata":{},"source":"### 2. Try least squares regression, anyway.  How well do you do?"},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":"X = nba[['Comp','Height','Points']]\ny = nba['Salary']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"data":{"text/plain":"0.18297542118405608"},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":"ols = LinearRegression().fit(X_train, y_train)\nols.score(X_test, y_test)"},{"cell_type":"markdown","metadata":{},"source":"As we can see from the model ouput, the performance is really really bad, given the accuracy score is less than 20%. We have enough confidence to say the linear model is not approproiate predicting player's salary in this case."},{"cell_type":"markdown","metadata":{},"source":"### 3-1 Build a model that predicts the probability of making it to the NBA."},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":"nba['Make_nba'] = np.where(nba['Salary']>0,1,0)\nX1 = nba[['Comp','Height','Points']]\ny1 = nba['Make_nba']"},{"cell_type":"markdown","metadata":{},"source":"### 3-2 Do a train-test split of 8000/2000 points, train your best model on the training set, and compute the AUC on the test set."},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"data":{"text/plain":"0.957"},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":"## We tried logistic regression\nX1_train, X1_test, y1_train, y1_test = train_test_split(X1, y1, test_size=0.2, random_state=42)\nlr = LogisticRegression(random_state=0, solver='lbfgs', multi_class='multinomial').fit(X1_train, y1_train)\nlr.score(X1_test,y1_test)"},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"data":{"text/plain":"0.952"},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":"## We tried random forest\nrf = RandomForestClassifier(n_estimators=4, max_depth=5).fit(X1_train, y1_train)\nrf.score(X1_test,y1_test)"},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"data":{"text/plain":"0.956"},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":"## We tried gradient boosting\ngb = GradientBoostingClassifier(n_estimators=30, max_depth=5).fit(X1_train, y1_train)\ngb.score(X1_test,y1_test)"},{"cell_type":"markdown","metadata":{},"source":"### We found logistic regression has the best performance, therefore, we choose logistic regression as our model."},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"The AUC on test set is 0.9554735022585427\n"}],"source":"roc=roc_auc_score(y1_test, lr.predict_proba(X1_test)[:,1])\nprint('The AUC on test set is '+str(roc))"},{"cell_type":"markdown","metadata":{},"source":"### 3-3 Now, build a model to predict the salary.  Note that you may wish to consider a non-linear transformation of your data. What is your R2 score on the test set?"},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Comp</th>\n      <th>Height</th>\n      <th>Points</th>\n      <th>Salary</th>\n      <th>Make_nba</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>9</td>\n      <td>9.0</td>\n      <td>78.0</td>\n      <td>63.0</td>\n      <td>496315.0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <td>27</td>\n      <td>10.0</td>\n      <td>76.0</td>\n      <td>52.0</td>\n      <td>741265.0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>8.0</td>\n      <td>78.0</td>\n      <td>62.0</td>\n      <td>292281.0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <td>115</td>\n      <td>10.0</td>\n      <td>77.0</td>\n      <td>63.0</td>\n      <td>782210.0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <td>117</td>\n      <td>8.0</td>\n      <td>76.0</td>\n      <td>50.0</td>\n      <td>259365.0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <td>9918</td>\n      <td>9.0</td>\n      <td>76.0</td>\n      <td>43.0</td>\n      <td>301898.0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <td>9928</td>\n      <td>10.0</td>\n      <td>79.0</td>\n      <td>47.0</td>\n      <td>949052.0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <td>9962</td>\n      <td>10.0</td>\n      <td>79.0</td>\n      <td>52.0</td>\n      <td>557551.0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <td>9967</td>\n      <td>10.0</td>\n      <td>78.0</td>\n      <td>54.0</td>\n      <td>751278.0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <td>9974</td>\n      <td>10.0</td>\n      <td>76.0</td>\n      <td>44.0</td>\n      <td>496271.0</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>587 rows × 5 columns</p>\n</div>","text/plain":"      Comp  Height  Points    Salary  Make_nba\n9      9.0    78.0    63.0  496315.0         1\n27    10.0    76.0    52.0  741265.0         1\n30     8.0    78.0    62.0  292281.0         1\n115   10.0    77.0    63.0  782210.0         1\n117    8.0    76.0    50.0  259365.0         1\n...    ...     ...     ...       ...       ...\n9918   9.0    76.0    43.0  301898.0         1\n9928  10.0    79.0    47.0  949052.0         1\n9962  10.0    79.0    52.0  557551.0         1\n9967  10.0    78.0    54.0  751278.0         1\n9974  10.0    76.0    44.0  496271.0         1\n\n[587 rows x 5 columns]"},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":"nbaplayer = nba[nba.Make_nba ==1]\nnbaplayer"},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[{"data":{"text/plain":"0.5415468455217294"},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":"## First we tried Standardize the data\nX_scaler = StandardScaler()\nX2 = nbaplayer[['Comp','Height','Points']]\ny2 = nbaplayer['Salary']\nX2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, test_size=0.2, random_state=42)\n\nX2_train_std = X_scaler.fit_transform(X2_train)\nX2_test_std = X_scaler.transform(X2_test)\n\nols = LinearRegression().fit(X2_train_std, y2_train)\ny_pred = ols.predict(X2_test_std)\nr2_score(y2_test, y_pred)"},{"cell_type":"markdown","metadata":{},"source":"### It seems like standardize the data doesn't really help us, so we tried log transformation."},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[],"source":"X2 = np.log(X2)\ny2 = np.log(y2)\nX2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, test_size=0.2, random_state=42)"},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"The r square score of logistic regression is 0.6307385310504816\n"}],"source":"## We tried logistic regression\nols = LinearRegression().fit(X2_train, y2_train)\ny_pred = ols.predict(X2_test)\nprint('The r square score of logistic regression is '+str(r2_score(y2_test, y_pred)))"},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"The r square score of random forest is 0.6040049393942528\n"}],"source":"## Again, we tried random forest\nfrom sklearn.ensemble import RandomForestRegressor\nrf1 = RandomForestRegressor(n_estimators=5, max_depth=5).fit(X2_train, y2_train)\ny_pred = rf1.predict(X2_test)\nprint('The r square score of random forest is '+str(r2_score(y2_test, y_pred)))"},{"cell_type":"markdown","metadata":{},"source":"### Again, we choose logistic regression since it has better performance."},{"cell_type":"markdown","metadata":{},"source":"### 4. Compute  the  expected  NBA  salary  of  a  high  school  basketball  player  who  is  6’  6”  tall,  is averaging 46 points per game, and is playing in the second most competitive league (comp =9), according to your model."},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[],"source":"## a player will make into NBA if the probability of making it to the NBA is higher than \n## the probablity parameter in this function\ndef playersal(height, points, league):\n    sal = ols.predict([[np.log(height),np.log(points), np.log(league)]])\n    expected_sal = math.exp(sal)*lr.predict_proba([[height,points, league]])[0][1]\n    print('The probablity of this play making NBA is '+str(lr.predict_proba([[height,points, league]])[0][1]))\n    print('The expected salary of this player in the NBA is '+ str(expected_sal))"},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"The probablity of this play making NBA is 0.10033141526796388\nThe expected salary of this player in the NBA is 46719.017820924346\n"}],"source":"playersal(9,78,46)"}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}